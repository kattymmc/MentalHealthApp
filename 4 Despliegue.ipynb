{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1a83f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ef481124e9437aa1d4722526f72994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 18:26:45 INFO: Downloading default packages for language: es (Spanish)...\n",
      "2022-01-20 18:26:48 INFO: File exists: C:\\Users\\USUARIO\\stanza_resources\\es\\default.zip.\n",
      "2022-01-20 18:26:51 INFO: Finished downloading models and saved to C:\\Users\\USUARIO\\stanza_resources.\n",
      "2022-01-20 18:26:51 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-01-20 18:26:51 INFO: Use device: cpu\n",
      "2022-01-20 18:26:51 INFO: Loading: tokenize\n",
      "2022-01-20 18:26:51 INFO: Loading: mwt\n",
      "2022-01-20 18:26:51 INFO: Loading: pos\n",
      "2022-01-20 18:26:51 INFO: Loading: lemma\n",
      "2022-01-20 18:26:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import stanza\n",
    "from joblib import dump, load\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fastapi import FastAPI\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stanza.download('es') \n",
    "# stopwords.words(\"spanish\")\n",
    "nlp = stanza.Pipeline('es', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "url = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "menciones = '@[\\w\\-]+'\n",
    "hashtag = '#[\\w\\-]+'\n",
    "caracteres_especiales = r'\\W'\n",
    "varios_espacios= r'\\s+'\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet_procesado = tweet.lower() \n",
    "    tweet_procesado = re.sub(menciones, ' ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(url, ' ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(caracteres_especiales, ' ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(\" q \", ' que ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(\" pq \", ' porque ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(\" xq \", ' porque ', tweet_procesado)\n",
    "    tweet_procesado = re.sub(varios_espacios, ' ', tweet_procesado, flags=re.I)\n",
    "    return tweet_procesado\n",
    "\n",
    "def procesar_texto(cadena):\n",
    "    palabras = cadena.split() \n",
    "    cadena_limpia = [palabra for palabra in palabras if palabra.lower() not in stopwords.words(\"spanish\")]\n",
    "    return cadena_limpia\n",
    "\n",
    "SVM = load('support-vector-machine.joblib')\n",
    "Naive = load('naive-bayes-classifier.joblib')\n",
    "RFC = load('random-forest-classifier.joblib')\n",
    "tfidf_vector = load('tfidf_vector.joblib')\n",
    "\n",
    "def predict_mental_issues(tweet, model):\n",
    "    cleaned = clean_tweet(tweet)\n",
    "    procesado = procesar_texto(cleaned)\n",
    "    texto_procesado = ' '.join(procesado)\n",
    "    doc = nlp(texto_procesado)\n",
    "    lemmas = []\n",
    "    for sent in doc.sentences :\n",
    "        for word in sent.words:\n",
    "            lemmas.append(word.lemma)\n",
    "    print(lemmas)\n",
    "    texto_final = ' '.join(lemmas)\n",
    "    Test_X_Tfidf = tfidf_vector.transform([texto_final])\n",
    "    print(Test_X_Tfidf)\n",
    "    if model == 'NB':\n",
    "        predictions = Naive.predict(Test_X_Tfidf)\n",
    "    elif model == 'SVM':\n",
    "        predictions = SVM.predict(Test_X_Tfidf)\n",
    "    elif model == 'RFC':\n",
    "        predictions = RFC.predict(Test_X_Tfidf)\n",
    "    else:\n",
    "        predictions = 'No se encontro el modelo'\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892d4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/mentalhealth\")\n",
    "def analyze_tweet(tweet: str, model: str):\n",
    "    try:\n",
    "        prediccion = predict_mental_issues(tweet, 'SVM')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return {'prediccion': prediccion[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b052cebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\uvicorn.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\click\\core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\main.py\", line 362, in main\n",
      "    run(**kwargs)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\main.py\", line 386, in run\n",
      "    server.run()\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\server.py\", line 49, in run\n",
      "    loop.run_until_complete(self.serve(sockets=sockets))\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 641, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\server.py\", line 56, in serve\n",
      "    config.load()\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\config.py\", line 308, in load\n",
      "    self.loaded_app = import_from_string(self.app)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\uvicorn\\importer.py\", line 20, in import_from_string\n",
      "    module = importlib.import_module(module_str)\n",
      "  File \"C:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"D:\\CICLO 10\\DESARROLLO DE TESIS II\\MentalHealthApp\\.\\main.py\", line 10, in <module>\n",
      "    nlp = stanza.Pipeline('es', processors='tokenize,mwt,pos,lemma')\n",
      "AttributeError: module 'stanza' has no attribute 'Pipeline'\n"
     ]
    }
   ],
   "source": [
    "!cd ./ && uvicorn main:app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d88be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
